{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于Jupyter的特征工程笔记本2: 特征选择**  \n",
    "*作者: 陈颖祥，杨子唅*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "- https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "- https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "- https://dcor.readthedocs.io/en/latest/functions/dcor.independence.distance_covariance_test.html\n",
    "- https://en.wikipedia.org/wiki/Distance_correlation\n",
    "- https://stats.stackexchange.com/questions/56881/whats-the-relationship-between-r2-and-f-test\n",
    "- https://en.wikipedia.org/wiki/Mutual_information\n",
    "- https://libguides.library.kent.edu/SPSS/ChiSquare\n",
    "- https://chrisalbon.com/machine_learning/feature_selection/anova_f-value_for_feature_selection/\n",
    "- https://online.stat.psu.edu/stat414/node/218/\n",
    "- http://featureselection.asu.edu/algorithms.php\n",
    "- https://en.wikipedia.org/wiki/Minimum_redundancy_feature_selection\n",
    "- Peng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis & Machine Intelligence, (8), 1226-1238.\n",
    "- Hall, M. A., & Smith, L. A. (1999, May). Feature selection for machine learning: comparing a correlation-based filter approach to the wrapper. In FLAIRS conference (Vol. 1999, pp. 235-239).\n",
    "- Yu, L., & Liu, H. (2003). Feature selection for high-dimensional data: A fast correlation-based filter solution. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 856-863).\n",
    "- Zhao, Z., & Liu, H. (2007, June). Spectral feature selection for supervised and unsupervised learning. In Proceedings of the 24th international conference on Machine learning (pp. 1151-1157). ACM.\n",
    "- Robnik-Šikonja, M., & Kononenko, I. (2003). Theoretical and empirical analysis of ReliefF and RReliefF. Machine learning, 53(1-2), 23-69.\n",
    "- https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "- http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理后，我们生成了大量的新变量（比如独热编码生成了大量仅包含0或1的变量）。但实际上，部分新生成的变量可能是多余：一方面它们本身不一定包含有用的信息，故无法提高模型性能；另一方面过这些多余变量在构建模型时会消耗大量内存和计算能力。因此，我们应该进行特征选择并选择特征子集进行建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods 过滤法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法通过使用一些统计量或假设检验结果为每个变量打分。得分较高的功能往往更加重要，因此应被包含在子集中。以下为一个简单的基于过滤法的机器学习工作流（以最简单的训练-验证-测试这种数据集划分方法为例）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./images/过滤法工作流.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Filter Methods 单变量特征过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单变量过滤方法依据单变量统计量或统计检验选择最佳特征。其仅仅考虑单个变量与目标变量的关系（方差选择法仅基于单个变量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Threshold 方差选择法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差选择法删除变量方差低于某个阈值的所有特征。例如，我们应删除方差为零的特征（所有观测点中具有相同值的特征），因为该特征无法解释目标变量的任何变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:01.158434Z",
     "start_time": "2020-03-15T22:22:59.849176Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 合成一些数据集用于演示\n",
    "train_set = np.array([[1,2,3],[1,4,7],[1,4,9]]) # 可见第一个变量方差为0\n",
    "# array([[1, 2, 3],\n",
    "#        [1, 4, 7],\n",
    "#        [1, 4, 9]])\n",
    "\n",
    "test_set = np.array([[3,2,3],[1,2,7]]) # 故意将第二个变量方差设为0\n",
    "# array([[3, 2, 3],\n",
    "#        [1, 2, 7]])\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(train_set) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "# 下面为返回结果，可见第一个变量已被删除\n",
    "# array([[2, 3],\n",
    "#        [4, 7],\n",
    "#        [4, 9]])\n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "# 下面为返回结果，可见第一个变量已被删除\n",
    "# array([[2, 3],\n",
    "#        [2, 7]])\n",
    "# 虽然测试集中第二个变量的方差也为0\n",
    "# 但是我们的选择是基于训练集，所以我们依然删除第一个变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Correlation (regression problem)  皮尔森相关系数 (回归问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "皮尔森相关系数一般用于衡量两个**连续**变量之间的线性相关性，也可以用于衡量二元变量与目标变量的相关性。故可以将类别变量利用独热编码转换为多个二元变量之后利用皮尔森相关系数进行筛选。\n",
    "\n",
    "公式：\n",
    "$r = \\frac{\\sum_{i=1}^{n}(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sqrt{(X_i-\\bar{X})^2}\\sqrt{(Y_i-\\bar{Y})^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:01.228185Z",
     "start_time": "2020-03-15T22:23:01.160662Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # 利用 california_housing 数据集来演示\n",
    "# 此数据集中，X，y均为连续变量，故此满足使用皮尔森相关系数的条件\n",
    "\n",
    "# 选择前15000个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# sklearn 中没有直接的方程可以使用\n",
    "# 此处将用 scipy.stats.pearsonr方程来实现基于皮尔森相关系数的特征过滤\n",
    "# 注意 scipy.stats.pearsonr 计算的是两个变量之间的相关系数\n",
    "# 因sklearn SelectKBest需要，我们将基于scipy.stats.pearsonr 重写允许多特征同时输入的方程 udf_pearsonr\n",
    "\n",
    "def udf_pearsonr(X, y):\n",
    "    # 将会分别计算每一个变量与目标变量的关系\n",
    "    result = np.array([pearsonr(x, y) for x in X.T]) # 包含(皮尔森相关系数, p值) 的列表\n",
    "    return np.absolute(result[:,0]), result[:,1] \n",
    "\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为皮尔森相关系数\n",
    "selector = SelectKBest(udf_pearsonr, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(15000, 2), 其选择了第一个及第七个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[0,6]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,6]]);\n",
    "# 可见对于测试集，其依然选择了第一个及第七个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:23:01.237188Z",
     "start_time": "2020-03-15T22:23:01.230256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量和目标的皮尔森相关系数的绝对值为0.7, p-值为0.0\n",
      "第2个变量和目标的皮尔森相关系数的绝对值为0.07, p-值为0.0\n",
      "第3个变量和目标的皮尔森相关系数的绝对值为0.14, p-值为0.0\n",
      "第4个变量和目标的皮尔森相关系数的绝对值为0.04, p-值为0.0\n",
      "第5个变量和目标的皮尔森相关系数的绝对值为0.02, p-值为0.011\n",
      "第6个变量和目标的皮尔森相关系数的绝对值为0.05, p-值为0.0\n",
      "第7个变量和目标的皮尔森相关系数的绝对值为0.23, p-值为0.0\n",
      "第8个变量和目标的皮尔森相关系数的绝对值为0.08, p-值为0.0\n"
     ]
    }
   ],
   "source": [
    "# 验算一下我们的结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    pea_score, p_value = pearsonr(train_set[:,idx], train_y)\n",
    "    print(f\"第{idx + 1}个变量和目标的皮尔森相关系数的绝对值为{round(np.abs(pea_score),2)}, p-值为{round(p_value,3)}\")\n",
    "# 应选择第一个及第七个变量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Correlation (regression problem) 距离相关系数 (回归问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与皮尔森相关系数类似，距离相关系数也一般被用于衡量两个连续变量之间的相关性。但与皮尔森相关系数不同的是，距离相关系数还衡量了两个变量之间的非线性关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式:  \n",
    "  \n",
    "首先，计算(n x n)距离矩阵dX。dX中的每一个元素为$dX_{ij}$ 。类似的，我们也可以计算距离矩阵dY，其中dY中的每个元素为$dY_{ij}$。$dX_{ij}$是为观测点i与观测点j之间的距离:     \n",
    "  \n",
    "$dX_{ij} = \\left \\| X_i - X_j \\right \\|$  \n",
    "$dY_{ij} = \\left \\| Y_i - Y_j \\right \\|$\n",
    "   \n",
    "其次，我们计算如下双中心距离并更新距离矩阵。其中，$\\bar{X_i}$为距离矩阵dX的第i行平均值，$\\bar{X_j}$为距离矩阵dX的第j列的平均值， $\\sum_i^N \\sum_j^N dX_{ij}$ 为全局平均值： \n",
    "  \n",
    "$dX_{ij} = dX_{ij} - \\bar{X_i} - \\bar{X_j} + \\frac{1}{N^2} \\sum_i^N \\sum_j^N dX_{ij}$  \n",
    "$dY_{ij} = dY_{ij} - \\bar{Y_i} - \\bar{Y_j} + \\frac{1}{N^2} \\sum_i^N \\sum_j^N dY_{ij}$   \n",
    "  \n",
    "随后，我们便可以算出样本距离协方差及距离方差：  \n",
    "  \n",
    "$dCov^2 (X, Y) = \\frac{1}{N^2} \\sum_{i}^{N} \\sum_{j}^{N} dX_{ij}dY_{ij}$  \n",
    "$dVar^2(X) = Cov^2_D (X, X)$  \n",
    "  \n",
    "最后，距离相关系数$dCor(X,Y)$ 便为如下： \n",
    "  \n",
    "$dCor(X,Y) = \\frac{dCov_D(X, Y)}{\\sqrt{dVar^2(X)}\\sqrt{dVar^2(Y)} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:25:02.234865Z",
     "start_time": "2020-03-15T22:23:01.239626Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dcor import distance_correlation\n",
    "from dcor.independence import distance_covariance_test\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # 利用 california_housing 数据集来演示\n",
    "# 此数据集中，X，y均为连续变量，故此满足使用距离相关系数的条件\n",
    "\n",
    "# 选择前15000个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# sklearn 中没有直接的方程可以使用\n",
    "# 此处将用 dcor.distance_correlation方程来实现基于距离相关系数的特征过滤\n",
    "# 注意 dcor.distance_correlation 计算的是两个变量之间的相关系数\n",
    "# 因sklearn SelectKBest需要，我们将基于dcor.distance_correlation 重写允许多特征同时输入的方程 udf_dcorr\n",
    "\n",
    "def udf_dcorr(X, y):\n",
    "    # 将会分别计算每一个变量与目标变量的关系\n",
    "    result = np.array([[distance_correlation(x, y), \n",
    "                        distance_covariance_test(x,y)[0]]for x in X.T]) # 包含(距离相关系数, p值) 的列表\n",
    "    return result[:,0], result[:,1]\n",
    "\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为距离相关系数\n",
    "selector = SelectKBest(udf_dcorr, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(15000, 2), 其选择了第一个及第三个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[0,2]])\n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,2]]);\n",
    "# 可见对于测试集，其依然选择了第一个及第三个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:28.829093Z",
     "start_time": "2020-03-15T22:25:02.256562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量和目标的距离相关系数为0.66, p-值为1.0\n",
      "第2个变量和目标的距离相关系数为0.07, p-值为1.0\n",
      "第3个变量和目标的距离相关系数为0.31, p-值为1.0\n",
      "第4个变量和目标的距离相关系数为0.12, p-值为1.0\n",
      "第5个变量和目标的距离相关系数为0.08, p-值为1.0\n",
      "第6个变量和目标的距离相关系数为0.29, p-值为1.0\n",
      "第7个变量和目标的距离相关系数为0.25, p-值为1.0\n",
      "第8个变量和目标的距离相关系数为0.19, p-值为1.0\n"
     ]
    }
   ],
   "source": [
    "# 验算一下我们的结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    d_score = distance_correlation(train_set[:,idx], train_y)\n",
    "    p_value = distance_covariance_test(train_set[:,idx], train_y)[0]\n",
    "    print(f\"第{idx + 1}个变量和目标的距离相关系数为{round(d_score,2)}, p-值为{round(p_value,3)}\")\n",
    "# 应选择第一个及第三个变量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:22:20.392522Z",
     "start_time": "2020-03-11T04:22:20.377296Z"
    }
   },
   "source": [
    "#### F-Score (regression problem) F-统计量 (回归问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F统计量（F-Score）用于检验线性回归模型的整体显著性。在sklearn中，其将对每一个变量分别建立一个一元的线性回归模型，然后分别报告每一个对应模型的F统计量。F-统计量的零假设是该线性模型系数不显著，在一元模型中，该统计量能够反映各变量与目标变量之间的线性关系。因此，我们应该选择具有较高F统计量的特征（更有可能拒绝原假设）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式：  \n",
    "  \n",
    "$F = \\frac{(SST - SSR)/(p - 1)}{SSR/(n - p)} =  \\frac{SST - SSR}{SSR/(n - 2)} =  \\frac{R^2}{(1 - R^2)(n - 2)} = \\frac{\\rho ^2}{(1 - \\rho ^2)(n - 2)}$  \n",
    " \n",
    "其中:  \n",
    "\n",
    "$SST = \\sum_{i=1}^{n}(y_i - \\overline{y}) ^2$  \n",
    "  \n",
    "$\\overline{y} = \\frac{1}{n} \\sum_{i=1}^{n}y_i$  \n",
    "  \n",
    "$SSR = \\sum_{i=1}^{n}(\\widehat{y}_i - \\overline{y})^2$  \n",
    "  \n",
    "$\\widehat{y}_i$ 为模型预测值\n",
    "  \n",
    "  \n",
    "SST为总平方和，SSR为回归平方和，p为线性回归自变量数（包括常数项，故在上述的一元线性模型中，p=2），$\\rho$为自变量与因变量的线性相关系数，n为总观测数。因上述线性模型为一元线性模型，故可证$\\rho^2 = R^2$。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:28.958744Z",
     "start_time": "2020-03-15T22:26:28.840956Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # 利用 california_housing 数据集来演示\n",
    "# 此数据集中，X，y均为连续变量，故此满足使用F统计量的条件\n",
    "\n",
    "# 选择前15000个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:15000,:]\n",
    "test_set = X[15000:,]\n",
    "train_y = y[0:15000]\n",
    "\n",
    "# sklearn 中直接提供了方程用于计算F统计量\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为F统计量\n",
    "selector = SelectKBest(f_regression, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(15000, 2), 其选择了第一个及第七个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[0,6]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,6]]);\n",
    "# 可见对于测试集，其依然选择了第一个及第七个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:28.973704Z",
     "start_time": "2020-03-15T22:26:28.960274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量的F统计量为14111.79, p-值为0.0\n",
      "第2个变量的F统计量为71.99, p-值为0.0\n",
      "第3个变量的F统计量为317.04, p-值为0.0\n",
      "第4个变量的F统计量为23.93, p-值为0.0\n",
      "第5个变量的F统计量为6.54, p-值为0.011\n",
      "第6个变量的F统计量为35.93, p-值为0.0\n",
      "第7个变量的F统计量为846.61, p-值为0.0\n",
      "第8个变量的F统计量为98.06, p-值为0.0\n"
     ]
    }
   ],
   "source": [
    "# 验算一下我们的结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = f_regression(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"第{idx + 1}个变量的F统计量为{round(score[0],2)}, p-值为{round(p_value[0],3)}\")\n",
    "# 故应选择第一个及第七个变量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information (regression problem) 互信息 (回归问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互信息（Mutual Information）衡量变量间的相互依赖性。其本质为熵差，即$H(X) - H(X|Y)，即知道另一个变量信息后混乱的降低程度$。当且仅当两个随机变量独立时MI等于零。MI值越高，两变量之间的相关性则越强。与Pearson相关和F统计量相比，它还捕获了非线性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式:  \n",
    "  \n",
    "- 若两个变量均为离散变量:  \n",
    "  \n",
    "    $I(x, y) = H(Y) - H(Y|X) = \\sum_{x\\in \\mathit{X}}  \\sum_{x\\in \\mathit{Y}} \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)})$  \n",
    "\n",
    "    $\\textit{p}_{(X,Y)}(x,y)$ 为x和y的联合概率质量函数 (PMF)， $\\textit{p}_{X}(x)$则为x的联合概率质量函数 (PMF)。  \n",
    "  \n",
    "- 若两个变量均为连续变量:  \n",
    "\n",
    "    $I(X, Y) = H(Y) - H(Y|X) = \\int_X \\int_Y  \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)}) \\, \\, dx dy$  \n",
    "    \n",
    "    $\\textit{p}_{(X,Y)}(x,y)$ 为x和y的联合概率密度函数 (PDF)，$\\textit{p}_{X}(x)$则为x的概率密度函数 (PDF)。连续变量情形下，在实际操作中，往往先对数据离散化分桶，然后逐个桶进行计算。\n",
    "  \n",
    "\n",
    "但是实际上，一种极有可能的情况是，x和y中的一个可能是离散变量，而另一个是连续变量。因此在sklearn中，它基于[1]和[2]中提出的基于k最临近算法的熵估计非参数方法。   \n",
    "   \n",
    "[1] A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.  \n",
    "[2] B. C. Ross “Mutual Information between Discrete and Continuous Data Sets”. PLoS ONE 9(2), 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:31.044024Z",
     "start_time": "2020-03-15T22:26:28.979923Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "X, y = dataset.data, dataset.target # 利用 california_housing 数据集来演示\n",
    "# 此数据集中，X，y均为连续变量，故此满足使用MI的条件\n",
    "\n",
    "# 选择前15000个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:15000,:].astype(float)\n",
    "test_set = X[15000:,].astype(float)\n",
    "train_y = y[0:15000].astype(float)\n",
    "\n",
    "# KNN中的临近数是一个非常重要的参数\n",
    "# 故我们重写了一个新的MI计算方程更好的来控制这一参数\n",
    "def udf_MI(X, y):\n",
    "    result = mutual_info_regression(X, y, n_neighbors = 5) # 用户可以输入想要的临近数\n",
    "    return result\n",
    "\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为F统计量\n",
    "selector = SelectKBest(udf_MI, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(15000, 2), 其选择了第一个及第八个变量\n",
    "assert np.array_equal(transformed_train, train_set[:,[0,7]])\n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[0,7]]);\n",
    "# 可见对于测试集，其依然选择了第一个及第八个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:32.966568Z",
     "start_time": "2020-03-15T22:26:31.047154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量与因变量的互信息为0.37\n",
      "第2个变量与因变量的互信息为0.03\n",
      "第3个变量与因变量的互信息为0.1\n",
      "第4个变量与因变量的互信息为0.03\n",
      "第5个变量与因变量的互信息为0.02\n",
      "第6个变量与因变量的互信息为0.09\n",
      "第7个变量与因变量的互信息为0.37\n",
      "第8个变量与因变量的互信息为0.46\n"
     ]
    }
   ],
   "source": [
    "# 验算上述结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score = mutual_info_regression(train_set[:,idx].reshape(-1,1), train_y, n_neighbors = 5)\n",
    "    print(f\"第{idx + 1}个变量与因变量的互信息为{round(score[0],2)}\")\n",
    "# 故应选择第一个及第八个变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared Statistics (classification problem) 卡方统计量 (分类问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卡方统计量主要用于衡量两个类别特征之间的相关性。sklearn提供了chi2方程用于计算卡方统计量。其输入的特征变量必须为布尔值或频率（故对于类别变量应考虑独热编码）。卡方统计量的零假设为两个变量是独立的，因为卡方统计量值越高，则两个类别变量的相关性越强。因此，我们应该选择具有较高卡方统计量的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式:  \n",
    "  \n",
    "$\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}} = n \\sum_{i,j} p_ip_j(\\frac{\\frac{O_{i,j}}{n} - p_i p_j}{p_i p_j})^2$  \n",
    "  \n",
    "其中，$O_{i,j}$为在变量X上具有i-th类别值且在变量Y上具有j-th类别值的实际观测点计数。$E_{i,j}$ 为利用概率估计的应在在变量X上具有i-th类别值且在变量Y上具有j-th类别值的观测点数量。n为总观测数，$p_i$为在变量X上具有i-th类别值的概率，$p_j$为在变量Y上具有j-th类别值的概率。  \n",
    "  \n",
    "**值得注意的是，通过解析源代码，我们发现在sklearn中利用chi2计算出来的卡方统计量并不是统计意义上的卡方统计量**。当输入变量为布尔变量时，chi2计算值为该布尔变量为True时候的卡方统计量（我们将会在下文举例说明）。这样的优势是，独热编码生成的所有布尔值变量的chi2值之和将等于原始变量统计意义上的卡方统计量。  \n",
    "  \n",
    "举个简单的例子，假设一个变量I有0，1，2两种可能的值，则独特编码后一共会产生3个新的布尔值变量。这三个布尔值变量的chi2计算出来的值之和，将等于变量I与因变量直接计算得出的统计意义上的卡方统计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 解析sklearn中chi2的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.022510Z",
     "start_time": "2020-03-15T22:26:32.968254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Output\n",
       "0     J       0\n",
       "1     J       1\n",
       "2     J       0\n",
       "3     B       2\n",
       "4     B       0\n",
       "5     B       1\n",
       "6     C       0\n",
       "7     C       0\n",
       "8     C       1\n",
       "9     C       2\n",
       "10    C       2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先，随机生成一个数据集\n",
    "import pandas as pd\n",
    "sample_dict = {'Type': ['J','J','J',\n",
    "                        'B','B','B',\n",
    "                        'C','C','C','C','C'], \n",
    "               'Output': [0, 1, 0, \n",
    "                          2, 0, 1,  \n",
    "                          0, 0, 1, 2, 2,]}\n",
    "sample_raw = pd.DataFrame(sample_dict)\n",
    "sample_raw #原始数据，Output是我们的目标变量，Type为类别变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.045322Z",
     "start_time": "2020-03-15T22:26:33.023979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.17777778, 0.42666667, 1.15555556]),\n",
       " array([0.91494723, 0.8078868 , 0.56114397]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面利用独热编码生成布尔变量，并利用sklearn计算每一个布尔变量的chi2值\n",
    "sample = pd.get_dummies(sample_raw)\n",
    "from sklearn.feature_selection import chi2\n",
    "chi2(sample.values[:,[1,2,3]],sample.values[:,[0]])\n",
    "# 第一行为每一个布尔变量的chi2值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.060308Z",
     "start_time": "2020-03-15T22:26:33.046806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Output</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>J</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Output  Count\n",
       "0    B       0      1\n",
       "1    B       1      1\n",
       "2    B       2      1\n",
       "3    C       0      2\n",
       "4    C       1      1\n",
       "5    C       2      2\n",
       "6    J       0      2\n",
       "7    J       1      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面直接计算原始变量Type与output统计学意义上的卡方统计量\n",
    "# 首先，先统计每一个类别下出现的观测数，用于创建列联表\n",
    "obs_df = sample_raw.groupby(['Type','Output']).size().reset_index()\n",
    "obs_df.columns = ['Type','Output','Count']\n",
    "obs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即列联表（contingency table）为：  \n",
    "\n",
    "| Type/Output | 0 | 1 | 2 |\n",
    "|------|------|------|------|\n",
    "|  B  | 1 | 1 | 1 |\n",
    "|  C  | 2 | 1 | 2 |\n",
    "|  J  | 2 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.067540Z",
     "start_time": "2020-03-15T22:26:33.061620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7600000000000002,\n",
       " 0.779791873961373,\n",
       " 4,\n",
       " array([[1.36363636, 0.81818182, 0.81818182],\n",
       "        [2.27272727, 1.36363636, 1.36363636],\n",
       "        [1.36363636, 0.81818182, 0.81818182]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "obs = np.array([[1, 1, 1], [2, 1, 2],[2, 1, 0]])\n",
    "chi2_contingency(obs) # 第一个值即为变量Type与output统计学意义上的卡方统计量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.073963Z",
     "start_time": "2020-03-15T22:26:33.068889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 而chi2方程算出来的布尔值之和为即为原始变量的统计意义上的卡方统计量\n",
    "chi2(sample.values[:,[1,2,3]],sample.values[:,[0]])[0].sum() == chi2_contingency(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.079655Z",
     "start_time": "2020-03-15T22:26:33.075231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=0.17777777777777778, pvalue=0.9149472287300311)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 那么sklearn中的chi2是如何计算的呢？\n",
    "# 不妨以第一个生成的布尔值为例，即Type为B\n",
    "# chi2出来的值为0.17777778\n",
    "# 而这与利用scipy以下代码计算出的计算一致\n",
    "from scipy.stats import chisquare\n",
    "f_exp = np.array([5/11, 3/11, 3/11]) * 3 # 预期频数为 output的先验概率 * Type为B 的样本数\n",
    "chisquare([1,1,1], f_exp=f_exp) # [1,1,1] 即Type为B 的样本实际频数\n",
    "# 即sklearn 中的chi2 仅考虑了Type为B情形下的列连表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 如何利用sklearn 来进行特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.091956Z",
     "start_time": "2020-03-15T22:26:33.081172Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import load_iris # 利用iris数据作为演示数据集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# 此数据集中，X为连续变量，y为类别变量\n",
    "# 不满足chi2的使用条件\n",
    "\n",
    "# 将连续变量变为布尔值变量以满足chi2使用条件\n",
    "# 不妨利用其是否大于均值来生成布尔值（仅作为演示用）\n",
    "X = X > X.mean(0)\n",
    "\n",
    "# iris 数据集使用前需要被打乱顺序\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# 选择前100个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# sklearn 中直接提供了方程用于计算卡方统计量\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为F统计量\n",
    "selector = SelectKBest(chi2, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(100, 2), 其选择了第三个及第四个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# 可见对于测试集，其依然选择了第三个及第四个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.100797Z",
     "start_time": "2020-03-15T22:26:33.093604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量与因变量的卡方统计量为29.69，p值为0.0\n",
      "第2个变量与因变量的卡方统计量为19.42，p值为0.0\n",
      "第3个变量与因变量的卡方统计量为31.97，p值为0.0\n",
      "第4个变量与因变量的卡方统计量为31.71，p值为0.0\n"
     ]
    }
   ],
   "source": [
    "# 验证上述结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = chi2(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"第{idx + 1}个变量与因变量的卡方统计量为{round(score[0],2)}，p值为{round(p_value[0],3)}\")\n",
    "# 故应选择第三个及第四个变量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Score (classification problem) F-统计量 (分类问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分类机器学习问题中，若变量特征为类别特征，则我们可以使用独热编码配合上述chi2方法选择最重要的特征。但若特征为连续变量，则我们可以使用ANOVA-F值。ANOVA F统计量的零假设是若按目标变量（类别）分组，则连续变量的总体均值是相同的。故我们应选择具有高ANOVA-F统计量的连续变量，因为这些连续变量与目标变量的关联性强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式：  \n",
    "  \n",
    "$F = \\frac{MSB}{MSE} = \\frac{ \\frac{SS(between)}{m-1}}{ \\frac{SS(error)}{n-m}}$   \n",
    "  \n",
    "其中，SS(between)为组间的平方和，即组均值和总体均值之间的平方和。 SS(error)是组内的平方和，即数据与组均值之间的平方和。 m是目标变量的总类别数，n是观测数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.110457Z",
     "start_time": "2020-03-15T22:26:33.102224Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import load_iris # 利用iris数据作为演示数据集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# 此数据集中，X为连续变量，y为类别变量\n",
    "# 满足ANOVA-F的使用条件\n",
    "\n",
    "# iris 数据集使用前需要被打乱顺序\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# 选择前100个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# sklearn 中直接提供了方程用于计算ANOVA-F\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为F统计量\n",
    "selector = SelectKBest(f_classif, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(100, 2), 其选择了第三个及第四个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# 可见对于测试集，其依然选择了第三个及第四个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.117338Z",
     "start_time": "2020-03-15T22:26:33.111672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量与因变量的ANOVA-F统计量为91.39，p值为0.0\n",
      "第2个变量与因变量的ANOVA-F统计量为33.18，p值为0.0\n",
      "第3个变量与因变量的ANOVA-F统计量为733.94，p值为0.0\n",
      "第4个变量与因变量的ANOVA-F统计量为608.95，p值为0.0\n"
     ]
    }
   ],
   "source": [
    "# 验证上述结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score, p_value = f_classif(train_set[:,idx].reshape(-1,1), train_y)\n",
    "    print(f\"第{idx + 1}个变量与因变量的ANOVA-F统计量为{round(score[0],2)}，p值为{round(p_value[0],3)}\")\n",
    "# 故应选择第三个及第四个变量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information (classification problem) 互信息 (分类问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【与1.1.1.5一样】互信息（Mutual Information）衡量变量间的相互依赖性。其本质为熵差，即$H(X) - H(X|Y)，即知道另一个变量信息后混乱的降低程度$。当且仅当两个随机变量独立时MI等于零。MI值越高，两变量之间的相关性则越强。与Pearson相关和F统计量相比，它还捕获了非线性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式:  \n",
    "  \n",
    "- 若两个变量均为离散变量:  \n",
    "  \n",
    "    $I(x, y) = H(Y) - H(Y|X) = \\sum_{x\\in \\mathit{X}}  \\sum_{x\\in \\mathit{Y}} \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)})$  \n",
    "\n",
    "    $\\textit{p}_{(X,Y)}(x,y)$ 为x和y的联合概率质量函数 (PMF)， $\\textit{p}_{X}(x)$则为x的的联合概率质量函数 (PMF)。  \n",
    "  \n",
    "- 若两个变量均为连续变量:  \n",
    "\n",
    "    $I(X, Y) = H(Y) - H(Y|X) = \\int_X \\int_Y  \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)}) \\, \\, dx dy$  \n",
    "    \n",
    "    $\\textit{p}_{(X,Y)}(x,y)$ 为x和y的联合概率密度函数 (PDF)，$\\textit{p}_{X}(x)$则为x的的联合概率密度函数 (PDF)。连续变量情形下，在实际操作中，往往先对数据离散化分桶，然后逐个桶进行计算。\n",
    "  \n",
    "\n",
    "但是实际上，一种极有可能的情况是，x和y中的一个可能是离散变量，而另一个是连续变量。因此在sklearn中，它基于[1]和[2]中提出的基于k最临近算法的熵估计非参数方法。   \n",
    "   \n",
    "[1] A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.  \n",
    "[2] B. C. Ross “Mutual Information between Discrete and Continuous Data Sets”. PLoS ONE 9(2), 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.136953Z",
     "start_time": "2020-03-15T22:26:33.118715Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 直接载入数据集\n",
    "from sklearn.datasets import load_iris # 利用iris数据作为演示数据集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# 此数据集中，X为连续变量，y为类别变量\n",
    "# 满足MI的使用条件\n",
    "\n",
    "# iris 数据集使用前需要被打乱顺序\n",
    "np.random.seed(1234)\n",
    "idx = np.random.permutation(len(X))\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# 选择前100个观测点作为训练集\n",
    "# 剩下的作为测试集\n",
    "train_set = X[0:100,:]\n",
    "test_set = X[100:,]\n",
    "train_y = y[0:100]\n",
    "\n",
    "# KNN中的临近数是一个非常重要的参数\n",
    "# 故我们重写了一个新的MI计算方程更好的来控制这一参数\n",
    "def udf_MI(X, y):\n",
    "    result = mutual_info_classif(X, y, n_neighbors = 5) # 用户可以输入想要的临近数\n",
    "    return result\n",
    "\n",
    "# SelectKBest 将会基于一个判别方程自动选择得分高的变量\n",
    "# 这里的判别方程为F统计量\n",
    "selector = SelectKBest(udf_MI, k=2) # k => 我们想要选择的变量数\n",
    "selector.fit(train_set, train_y) # 在训练集上训练\n",
    "transformed_train = selector.transform(train_set) # 转换训练集\n",
    "transformed_train.shape #(100, 2), 其选择了第三个及第四个变量 \n",
    "assert np.array_equal(transformed_train, train_set[:,[2,3]]) \n",
    "\n",
    "transformed_test = selector.transform(test_set) # 转换测试集\n",
    "assert np.array_equal(transformed_test, test_set[:,[2,3]]);\n",
    "# 可见对于测试集，其依然选择了第三个及第四个变量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T22:26:33.154500Z",
     "start_time": "2020-03-15T22:26:33.138529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个变量与因变量的互信息为0.56\n",
      "第2个变量与因变量的互信息为0.28\n",
      "第3个变量与因变量的互信息为0.99\n",
      "第4个变量与因变量的互信息为1.02\n"
     ]
    }
   ],
   "source": [
    "# 验算上述结果\n",
    "for idx in range(train_set.shape[1]):\n",
    "    score = mutual_info_classif(train_set[:,idx].reshape(-1,1), train_y, n_neighbors = 5)\n",
    "    print(f\"第{idx + 1}个变量与因变量的互信息为{round(score[0],2)}\")\n",
    "# 故应选择第三个及第四个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
